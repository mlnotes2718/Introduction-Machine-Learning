{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7734d10a",
   "metadata": {},
   "source": [
    "# Machine Learning Master Notes 10 - Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c182ce",
   "metadata": {},
   "source": [
    "### Prepare Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cec6739",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib import cm\n",
    "\n",
    "# SciKit Learn Regression Model\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# SciKit Learn Pre-processing and Feature Scaling\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# The following file contain the finalized gradient descent, cost function program \n",
    "import MyRegressionProgramV1 as my"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936b3077",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression: Cost Function and Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32bdbc0",
   "metadata": {},
   "source": [
    "Hypothesis: $$f_{\\vec{w},b}(\\vec{X}^{(i)})=b + \\sum\\limits_{j=0}^{n-1} \\vec{w}_{j}\\vec{X}_{j}^{(i)}$$\n",
    "\n",
    "\n",
    "Cost Function:\t$$J(\\vec w, b) = \\frac{1}{2m}   \\sum\\limits_{i=0}^{m-1} (f_{\\vec w,b}(\\vec{X}^{(i)})-\\vec y^{(i)})^{2}$$ \n",
    "$$J(\\vec w, b) = \\frac{1}{2m} \\sum\\limits_{i=0}^{m-1} \\left(\\left(b + \\sum\\limits_{j=0}^{n-1} \\vec w_{j} \\vec X_{j}^{(i)} \\right)-\\vec y^{(i)}\\right)^{2}$$\n",
    "$$J(\\vec w, b) = \\frac{1}{2m} \\sum\\limits_{i=0}^{m-1} \\left(\\left(b + \\vec X^{(i)} \\cdot \\vec w \\right)-\\vec y^{(i)}\\right)^{2}$$\n",
    "$$$$\n",
    "Gradient Descent Algorithm: $$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\n",
    "\\;  \\vec w &= \\vec w -  \\alpha \\frac{\\partial J(\\vec{w},b)}{\\partial \\vec{w}}  \\; \\newline \n",
    " b &= b -  \\alpha \\frac{\\partial J(\\vec{w},b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "\n",
    "\n",
    "Partial Derivatives: $$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(\\vec{w},b)}{\\partial \\vec{w}}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\vec{w},b}(\\vec{X}^{(i)}) - y^{(i)})\\vec{X}^{(i)} \\\\\n",
    "  \\frac{\\partial J(\\vec{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\vec{w},b}(\\vec{X}^{(i)}) - y^{(i)}) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Full Implementation of Gradient Descent:\n",
    "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\n",
    "\\;  \\vec{w} &= \\vec{w} -  \\alpha \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} ((\\vec{X}^{(i)}\\cdot \\vec{w} + b) - y^{(i)})\\vec{X}^{(i)}  \\; \\newline \n",
    " b &= b -  \\alpha \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} ((\\vec{X}^{(i)}\\cdot \\vec{w} + b) - y^{(i)})  \\newline \\rbrace\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb28c4a",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170cb590-d08d-4a40-bcca-ba2979cd3823",
   "metadata": {},
   "source": [
    "### Why Do We Need Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2e09ba",
   "metadata": {},
   "source": [
    "Assume that when we use linear regression to predict housing prices, we have floor area in feet which range from 800 to 2000 sq foot. We also use the number of rooms which range from 1 to 5. When comparing with house size and number of rooms, the scale differs too much.\n",
    "\n",
    "When using gradient descent algorithm, the program will compute slowly since we are working on different scales for each feature. The gradient descent algorithm will converge much faster when there are on the same scale.\n",
    "\n",
    "From the previous example we worked before, we also encounter Python overflow errors as the numbers we deal with are very large. Using feature scaling, we can reduce the number so that it is more manageable.\n",
    "\n",
    "The idea of feature scaling is to change the scale for each feature so that they are in a similar range. We can do so for example by dividing the number of square feet by 2000 and we can divide the number of rooms by 5. This will bring the two different scales into a similar range.\n",
    "\n",
    "One of a main technique in feature scaling is to re-scaled all features into a number between 0 and 1. This technique is called Normalization. A common method of normalization feature scaling is MinMax Scaling.\n",
    "\n",
    "Another feature scaling method is to convert the features data such that the standard deviation is 1 and the mean is 0. This type of scaling is suitable for dataset that are in Gaussian distribution form. This technique is also known as standardization. Common method for standardization is z-score scaling.\n",
    "\n",
    "In summary, feature scaling provides the following advantages:\n",
    "\n",
    "- **Features scaling allows gradient descent to converge faster and thus enhancing the performance of machine learning.**\n",
    "- **Features scaling can also address outliers problem by converting the distribution into Gaussian distribution.**\n",
    "- **Features scaling also helps to balance the impact of larger scale features against smaller scaled features.**   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51b17a5-4c3e-4170-8c78-d5116f2dc6c7",
   "metadata": {},
   "source": [
    "**Additional Reference**\n",
    "\n",
    "- https://medium.com/@shivanipickl/what-is-feature-scaling-and-why-does-machine-learning-need-it-104eedebb1c9\n",
    "- https://www.baeldung.com/cs/normalization-vs-standardization\n",
    "- https://machinelearningmastery.com/standardscaler-and-minmaxscaler-transforms-in-python/\n",
    "- https://towardsdatascience.com/all-about-feature-scaling-bcc0ad75cb35\n",
    "- https://www.blog.trainindata.com/feature-scaling-in-machine-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df237a4-47a0-4dd3-8bfc-70612e3f5ce8",
   "metadata": {},
   "source": [
    "### Illustration of Feature Scaling with Training Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83e866c-352f-4126-aacf-318c2a8eb4b1",
   "metadata": {},
   "source": [
    "The following training dataset contains three examples with four features (size, bedrooms, floors and, age) shown in the table below. \n",
    "\n",
    "| Size (sqft) | Number of Bedrooms  | Number of floors | Age of  Home | Price (1000s dollars)  |   \n",
    "| ----------------| ------------------- |----------------- |--------------|-------------- |  \n",
    "| 2104            | 5                   | 1                | 45           | 460           |  \n",
    "| 1416            | 3                   | 2                | 40           | 232           |  \n",
    "| 852             | 2                   | 1                | 35           | 178           |  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd522374-f0d3-4ef4-bb31-f2aa70e5c4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]]).reshape((3,4))\n",
    "y_train1 = np.array([460, 232, 178]).reshape((3,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a24a095b-943a-49e7-900f-77f3022eed9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aiml/Documents/GitHub/Machine-Learning-Master-Notes/MyRegressionProgramV1.py:30: RuntimeWarning: overflow encountered in square\n",
      "  lossFunction = (fx - y) ** 2\n",
      "/Users/aiml/Documents/GitHub/Machine-Learning-Master-Notes/MyRegressionProgramV1.py:29: RuntimeWarning: overflow encountered in matmul\n",
      "  fx = (X@w)+b\n",
      "/Users/aiml/Documents/GitHub/Machine-Learning-Master-Notes/MyRegressionProgramV1.py:63: RuntimeWarning: overflow encountered in matmul\n",
      "  fx = ((X@w) + b)\n",
      "/Users/aiml/Documents/GitHub/Machine-Learning-Master-Notes/MyRegressionProgramV1.py:174: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  b = b - (alpha * db)\n",
      "/Users/aiml/Documents/GitHub/Machine-Learning-Master-Notes/MyRegressionProgramV1.py:175: RuntimeWarning: invalid value encountered in subtract\n",
      "  w = w - (alpha * dw)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 9999: Last cost = nan: intercept = nan: weights = [[nan nan nan nan]]\n",
      "best w [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "best b nan\n"
     ]
    }
   ],
   "source": [
    "w, b, cost_history, w_history, b_history = my.compute_gradient_descent(X_train1, y_train1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "33c8ec72-4840-489a-9347-48b4aaf43ea2",
   "metadata": {},
   "source": [
    "**In this case the square foot is too large for computation. Thus we have computation error.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904c5d6a-ee04-4978-a316-96409da951a6",
   "metadata": {},
   "source": [
    "Let us re-scale the data by diving the house size by 1000 and the age of the house by 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf45e08a-1441-4a6e-b8ac-9e19fbfea9e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.104, 5.   , 1.   , 4.5  ],\n",
       "       [1.416, 3.   , 2.   , 4.   ],\n",
       "       [0.852, 2.   , 1.   , 3.5  ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled1 = X_train1 * np.array([0.001,1,1,0.1]).reshape((1,4))\n",
    "X_scaled1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70f5a92e-7b9a-4d60-b2a0-3e91419472e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 9999: Last cost = 5.8545e-11: intercept = -1.9716e+00: weights = [[ 26.30148126  78.60186731 -46.06835442  13.26495614]]\n",
      "best w [[ 26.3015]\n",
      " [ 78.6019]\n",
      " [-46.0684]\n",
      " [ 13.265 ]]\n",
      "best b -1.9716\n"
     ]
    }
   ],
   "source": [
    "w, b, cost_history, w_history, b_history = my.compute_gradient_descent(X_scaled1, y_train1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fff964d-fc5d-4c94-b118-beadf81ca1b9",
   "metadata": {},
   "source": [
    "## Methods of Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d45c74",
   "metadata": {},
   "source": [
    "### Method 1: Max Scaling (Divide by Max Value in the Range)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90535980",
   "metadata": {},
   "source": [
    "There are multiple ways to rescale the `x` features. First method is to use `x` and divide by the max number in the range of `x`. The formula is as follows:\n",
    "\n",
    "$$x_{(scaled)} = \\frac{x}{max(x)}$$\n",
    "\n",
    "Example:\n",
    "\n",
    "Using the example above:\n",
    "\n",
    "If $x_1$ refers to square feet of a house and it ranges from $852<x_1<2104$.  \n",
    "\n",
    "Then we need to divide $x$ by $2104$. After the conversion, the range should be as follows:\n",
    "\n",
    "$$0.4<x_{1\\_scaled}<1$$\n",
    "\n",
    "If $x_2$ refers to number of rooms in a house and it ranges from $2<x_2<5$. Then:\n",
    "\n",
    "$$0.4<x_{2\\_scaled}<1$$\n",
    "\n",
    "In the examples above, we can re-scaled the training data to the similar range so that they converge faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17a4d370-9af0-4229-9d46-fb02da407cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2104, 1416,  852])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1 = X_train1[:,0]\n",
    "x_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5bc61cb-fe8a-4410-895e-e67ac74ea042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 3, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_2 = X_train1[:,1]\n",
    "x_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6381c27-e09f-4f3f-ab5d-539e44ada90e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.6730038 , 0.40494297])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1_scaled = x_1/max(x_1)\n",
    "x_1_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58a547b9-87f8-4b4f-a6c1-aa6e38839fd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1. , 0.6, 0.4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_2_scaled = x_2/max(x_2)\n",
    "x_2_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65a7ba9a-7290-4339-942a-864483490613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2104,    5,    1,   45],\n",
       "       [1416,    3,    2,   40],\n",
       "       [ 852,    2,    1,   35]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa5ec96b-0caa-4da4-82be-97d55a96e816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2104,    5,    2,   45])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.amax(X_train1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3ae9572-81e2-40fd-a601-594c622645fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 1.        , 0.5       , 1.        ],\n",
       "       [0.6730038 , 0.6       , 1.        , 0.88888889],\n",
       "       [0.40494297, 0.4       , 0.5       , 0.77777778]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled1 = X_train1/np.amax(X_train1, axis=0)\n",
    "X_scaled1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4de4eb-48bd-4296-bb4f-c8d1ff5d765a",
   "metadata": {},
   "source": [
    "The entire array will re-scaled between 0.4 and 1.\n",
    "\n",
    "| Size (sqft) | Number of Bedrooms  | Number of floors | Age of  Home | Price (1000s dollars)  |   \n",
    "| ----------------| ------------------- |----------------- |--------------|-------------- |  \n",
    "| 1           | 1                   | 0.5               | 1.          | 460           |  \n",
    "| 0.6730038            | 0.6                   | 1                | 0.88888889           | 232           |  \n",
    "| 0.40494297             | 0.4                   | 0.5                | 0.77777778          | 178           |  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cebe7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_scaler(X):\n",
    "    \"\"\"\n",
    "    This function is max scaler.\n",
    "    Formula is x(scaled) = x / (max of x)\n",
    "    This function produce similar result as sklearn normalizer max\n",
    "    \"\"\"\n",
    "    maxValue = np.amax(X, axis=0)\n",
    "    scaled = X/maxValue\n",
    "    return scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93e24cef-f503-4f5e-85fc-fc4d1235a505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         1.         0.5        1.        ]\n",
      " [0.6730038  0.6        1.         0.88888889]\n",
      " [0.40494297 0.4        0.5        0.77777778]]\n"
     ]
    }
   ],
   "source": [
    "# Our function\n",
    "print(max_scaler(X_train1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67c4f846-c598-4189-bb93-8cd3d4046731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         1.         0.5        1.        ]\n",
      " [0.6730038  0.6        1.         0.88888889]\n",
      " [0.40494297 0.4        0.5        0.77777778]]\n"
     ]
    }
   ],
   "source": [
    "# Similar scaling function in SciKit Learn\n",
    "# Please note that we need to import preprocessing\n",
    "from sklearn import preprocessing\n",
    "print(preprocessing.normalize(X_train1, norm='max', axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32925b6-9859-40ad-bfd0-f50abf4d35cc",
   "metadata": {},
   "source": [
    "#### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dace68e0-0dd8-459f-8dae-aa0bf8ee15cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_array2 = np.array([2,3,5,6,7,4,8,7,6]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18a30886-f5ca-4c7c-a73e-e92be8ee9cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.25 ]\n",
      " [0.375]\n",
      " [0.625]\n",
      " [0.75 ]\n",
      " [0.875]\n",
      " [0.5  ]\n",
      " [1.   ]\n",
      " [0.875]\n",
      " [0.75 ]]\n"
     ]
    }
   ],
   "source": [
    "# Our function\n",
    "print(max_scaler(x_array2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1569b123-9d7f-4c00-ab53-dee92af4ef72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.25 ]\n",
      " [0.375]\n",
      " [0.625]\n",
      " [0.75 ]\n",
      " [0.875]\n",
      " [0.5  ]\n",
      " [1.   ]\n",
      " [0.875]\n",
      " [0.75 ]]\n"
     ]
    }
   ],
   "source": [
    "# Similar scaling function in SciKit Learn\n",
    "print(preprocessing.normalize(x_array2, norm='max', axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6821f7ae-0f59-4f99-acc9-df6bf96a7956",
   "metadata": {},
   "source": [
    "#### Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16777b34-dcae-4bc7-9ac7-7d53510f10a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.0e+03, 3.0e+00, 3.0e+01, 5.0e-01],\n",
       "       [1.0e+03, 4.0e+00, 4.5e+01, 4.0e-01],\n",
       "       [1.5e+03, 1.0e+00, 5.0e+01, 3.0e-01]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_array3 = np.array([[2000,3,30,0.5], [1000,4,45,0.4], [1500,1,50,0.3]])\n",
    "X_array3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2543fdf3-ab5a-4303-ba9c-2de0313a81f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.   0.75 0.6  1.  ]\n",
      " [0.5  1.   0.9  0.8 ]\n",
      " [0.75 0.25 1.   0.6 ]]\n"
     ]
    }
   ],
   "source": [
    "# Our function\n",
    "print(max_scaler(X_array3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8dc9e626-f268-49ca-ac70-bb3cb5fb7487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.   0.75 0.6  1.  ]\n",
      " [0.5  1.   0.9  0.8 ]\n",
      " [0.75 0.25 1.   0.6 ]]\n"
     ]
    }
   ],
   "source": [
    "# Similar scaling function in SciKit Learn\n",
    "print(preprocessing.normalize(X_array3, norm='max', axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d66df4",
   "metadata": {},
   "source": [
    "### Method 2: Mean Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac85da22",
   "metadata": {},
   "source": [
    "For Mean Normalization, the formula is\n",
    "\n",
    "$$x_{scaled} = \\frac{x - \\mu}{max(x)-min(x)}$$\n",
    "\n",
    "Using mean normalization, the data will range between a negative number and positive number. In our training example, the data will range from $-0.5>X>0.66$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a55dcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_norm(X):\n",
    "    \"\"\"\n",
    "    This function is mean normalizer.\n",
    "    Formula is x(scaled) = x - mean / (max of x) - (min of x)\n",
    "    There is no similar scaler in sklearn\n",
    "    \"\"\"\n",
    "    big = X.max(axis=0)\n",
    "    small = X.min(axis=0)\n",
    "    norm_range = big - small\n",
    "    avg = X.mean(axis=0)\n",
    "    scaled = (X - avg) / norm_range\n",
    "    return scaled, avg, norm_range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b3a969-8175-4248-b6e8-1b6f702e3687",
   "metadata": {},
   "source": [
    "#### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c7ff024-39fd-4777-9d4f-819644e294b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.51650692  0.55555556 -0.33333333  0.5       ]\n",
      " [-0.03301384 -0.11111111  0.66666667  0.        ]\n",
      " [-0.48349308 -0.44444444 -0.33333333 -0.5       ]]\n"
     ]
    }
   ],
   "source": [
    "# Our function\n",
    "X_norm1, _, _ = mean_norm(X_train1)\n",
    "print(X_norm1)\n",
    "# There is no similar scaling function in SciKit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2080e4-ba73-421d-87e3-534f7c886acc",
   "metadata": {},
   "source": [
    "#### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca82d9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.55555556]\n",
      " [-0.38888889]\n",
      " [-0.05555556]\n",
      " [ 0.11111111]\n",
      " [ 0.27777778]\n",
      " [-0.22222222]\n",
      " [ 0.44444444]\n",
      " [ 0.27777778]\n",
      " [ 0.11111111]]\n"
     ]
    }
   ],
   "source": [
    "x_array2 = np.array([2,3,5,6,7,4,8,7,6]).reshape(-1,1)\n",
    "\n",
    "# Our function\n",
    "x_norm2, _, _ = mean_norm(x_array2)\n",
    "print(x_norm2)\n",
    "# There is no similar scaling function in SciKit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158651be-5155-42aa-8f27-132dc7ece661",
   "metadata": {},
   "source": [
    "#### Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6398e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.00000000e-01  1.11111111e-01 -5.83333333e-01  5.00000000e-01]\n",
      " [-5.00000000e-01  4.44444444e-01  1.66666667e-01  2.77555756e-16]\n",
      " [ 0.00000000e+00 -5.55555556e-01  4.16666667e-01 -5.00000000e-01]]\n"
     ]
    }
   ],
   "source": [
    "X_array3 = np.array([[2000,3,30,0.5], [1000,4,45,0.4], [1500,1,50,0.3]])\n",
    "# Our function\n",
    "X_norm3, _, _ = mean_norm(X_array3)\n",
    "print(X_norm3)\n",
    "# There is no similar scaling function in SciKit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080629c7",
   "metadata": {},
   "source": [
    "### Method 3: Z-Score Normalization (Standardization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bb1070",
   "metadata": {},
   "source": [
    "For Z-Score Normalization, the formula is\n",
    "\n",
    "$$x_{scaled} = \\frac{x - \\mu}{\\sigma}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a562b41-7b03-4fba-a487-05653b0db766",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>\n",
    "Please note that Numpy and Pandas use different definitions of standard deviation (sample vs. population standard deviation). The result may be slightly different. More will be explained below.\n",
    "</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b9b7b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_norm_v1(X):\n",
    "    \"\"\"\n",
    "    This function is z-score normalizer.\n",
    "    Formula is x(scaled) = x - mean / {std deviation of x}\n",
    "    There is similar scaler in sklearn is StandardScaler\n",
    "    \"\"\"\n",
    "    \n",
    "    avg = X.mean(axis=0)\n",
    "\n",
    "    std = X.std(axis=0)\n",
    "    \n",
    "    X_norm = (X-avg)/std\n",
    "    \n",
    "    return X_norm, avg, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6240a1bd-f81e-4753-972d-0a8794cb2865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.26311506  1.33630621 -0.70710678  1.22474487]\n",
      " [-0.08073519 -0.26726124  1.41421356  0.        ]\n",
      " [-1.18237987 -1.06904497 -0.70710678 -1.22474487]]\n"
     ]
    }
   ],
   "source": [
    "# Our function\n",
    "X_norm1, _, _ = std_norm_v1(X_train1)\n",
    "print(X_norm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3c562c7-1bc3-44ab-a213-e140efb9fec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.26311506  1.33630621 -0.70710678  1.22474487]\n",
      " [-0.08073519 -0.26726124  1.41421356  0.        ]\n",
      " [-1.18237987 -1.06904497 -0.70710678 -1.22474487]]\n"
     ]
    }
   ],
   "source": [
    "# Similar scaling function in SciKit Learn\n",
    "std_scaler = StandardScaler().fit(X_train1)\n",
    "normalized_arr = std_scaler.transform(X_train1)\n",
    "print(normalized_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0f93ba-8e2f-48aa-8866-959d342aa2ae",
   "metadata": {},
   "source": [
    "#### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f274f1e-417e-473e-9532-4c37eb27b4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_array2 = np.array([2,3,5,6,7,4,8,7,6]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0bc9d99a-3ff6-4e2a-8704-9beb9c4a7320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.76776695]\n",
      " [-1.23743687]\n",
      " [-0.1767767 ]\n",
      " [ 0.35355339]\n",
      " [ 0.88388348]\n",
      " [-0.70710678]\n",
      " [ 1.41421356]\n",
      " [ 0.88388348]\n",
      " [ 0.35355339]]\n"
     ]
    }
   ],
   "source": [
    "# Our function\n",
    "x_norm2, _, _ = std_norm_v1(x_array2)\n",
    "print(x_norm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cbf42ba2-8f95-455b-9113-e229063f3bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.76776695]\n",
      " [-1.23743687]\n",
      " [-0.1767767 ]\n",
      " [ 0.35355339]\n",
      " [ 0.88388348]\n",
      " [-0.70710678]\n",
      " [ 1.41421356]\n",
      " [ 0.88388348]\n",
      " [ 0.35355339]]\n"
     ]
    }
   ],
   "source": [
    "# Similar scaling function in SciKit Learn\n",
    "std_scaler = StandardScaler().fit(x_norm2)\n",
    "normalized_arr = std_scaler.transform(x_norm2)\n",
    "print(normalized_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36a6ebf-25f6-4f06-8a80-45b3773e85ee",
   "metadata": {},
   "source": [
    "#### Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2d6c253c-3594-47fe-b775-c7b7c9045709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.0e+03, 3.0e+00, 3.0e+01, 5.0e-01],\n",
       "       [1.0e+03, 4.0e+00, 4.5e+01, 4.0e-01],\n",
       "       [1.5e+03, 1.0e+00, 5.0e+01, 3.0e-01]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_array3 = np.array([[2000,3,30,0.5], [1000,4,45,0.4], [1500,1,50,0.3]])\n",
    "X_array3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "28338932-6fb1-4bcc-b79a-1518a03c8b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.22474487e+00  2.67261242e-01 -1.37281295e+00  1.22474487e+00]\n",
      " [-1.22474487e+00  1.06904497e+00  3.92232270e-01  6.79869978e-16]\n",
      " [ 0.00000000e+00 -1.33630621e+00  9.80580676e-01 -1.22474487e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Our function\n",
    "X_norm3, _, _ = std_norm_v1(X_array3)\n",
    "print(X_norm3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6d23b6c9-bfcf-444e-a7d6-9d63b0d5d22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.22474487e+00  2.67261242e-01 -1.37281295e+00  1.22474487e+00]\n",
      " [-1.22474487e+00  1.06904497e+00  3.92232270e-01  6.79869978e-16]\n",
      " [ 0.00000000e+00 -1.33630621e+00  9.80580676e-01 -1.22474487e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Similar scaling function in SciKit Learn\n",
    "std_scaler = StandardScaler().fit(X_array3)\n",
    "normalized_arr = std_scaler.transform(X_array3)\n",
    "print(normalized_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc05b35c",
   "metadata": {},
   "source": [
    "**The above confirm that our functions and SciKit Learn function products same results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9fd079-4088-44e6-a6a2-d5e627512452",
   "metadata": {},
   "source": [
    "## SciKit Learn Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9711bc43",
   "metadata": {},
   "source": [
    "SKlearn prodives a list of scaling methods:\n",
    "- StandardScaler\n",
    "- MinMaxScaler\n",
    "- MaxAbsScaler\n",
    "- RobustScaler\n",
    "- PowerTransformer\n",
    "- QuantileTransformer\n",
    "- Normalizer\n",
    "\n",
    "We will only be looking at:\n",
    "- StandardScaler\n",
    "- MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06265f0e",
   "metadata": {},
   "source": [
    "### StandardScaler (Same as Z-Score Normalization - Standardization Technique)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18911352",
   "metadata": {},
   "source": [
    "In SciKit Learn StandardScaler use the formula below which is the same as our Z-Score Normalization formula:\n",
    "\n",
    "$$x_{scaled} = \\frac{x - \\mu}{\\sigma}$$\n",
    "$$ $$\n",
    "The effect of this scaling technique is to have a **mean of 0** and **standard deviation of 1**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "70a38d22-b8c6-4753-b969-497eae7ed7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.26311506  1.33630621 -0.70710678  1.22474487]\n",
      " [-0.08073519 -0.26726124  1.41421356  0.        ]\n",
      " [-1.18237987 -1.06904497 -0.70710678 -1.22474487]]\n"
     ]
    }
   ],
   "source": [
    "# Our function\n",
    "X_norm1, _, _ = std_norm_v1(X_train1)\n",
    "print(X_norm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c92b8973-d02c-4955-ac8b-88103203ab7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.26311506  1.33630621 -0.70710678  1.22474487]\n",
      " [-0.08073519 -0.26726124  1.41421356  0.        ]\n",
      " [-1.18237987 -1.06904497 -0.70710678 -1.22474487]]\n"
     ]
    }
   ],
   "source": [
    "# Similar scaling function in SciKit Learn\n",
    "std_scaler = StandardScaler().fit(X_train1)\n",
    "normalized_arr = std_scaler.transform(X_train1)\n",
    "print(normalized_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3502648-1dbe-4172-8c90-a1346d5a5a9d",
   "metadata": {},
   "source": [
    "#### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "094c44e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.76776695]\n",
      " [-1.23743687]\n",
      " [-0.1767767 ]\n",
      " [ 0.35355339]\n",
      " [ 0.88388348]\n",
      " [-0.70710678]\n",
      " [ 1.41421356]\n",
      " [ 0.88388348]\n",
      " [ 0.35355339]]\n"
     ]
    }
   ],
   "source": [
    "x_array2 = np.array([2,3,5,6,7,4,8,7,6]).reshape(-1,1)\n",
    "\n",
    "# StandardScaler in SciKit Learn\n",
    "std_scaler = StandardScaler().fit(x_array2)\n",
    "normalized_arr = std_scaler.transform(x_array2)\n",
    "print(normalized_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "51bc7020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaler mean [5.33333333]\n",
      "scaler std deviation [1.88561808]\n"
     ]
    }
   ],
   "source": [
    "print('scaler mean',std_scaler.mean_)\n",
    "print('scaler std deviation',std_scaler.scale_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f5f94ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.76776695]\n",
      " [-1.23743687]\n",
      " [-0.1767767 ]\n",
      " [ 0.35355339]\n",
      " [ 0.88388348]\n",
      " [-0.70710678]\n",
      " [ 1.41421356]\n",
      " [ 0.88388348]\n",
      " [ 0.35355339]]\n"
     ]
    }
   ],
   "source": [
    "# Our function that is same as StandardScaler\n",
    "x_norm2, avg, stddev = std_norm_v1(x_array2)\n",
    "print(x_norm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fd86ade1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our function mean [5.33333333]\n",
      "our function std deviation [1.88561808]\n"
     ]
    }
   ],
   "source": [
    "print('our function mean',avg)\n",
    "print('our function std deviation',stddev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24db7c83-a8c1-47d5-bd3d-d7eaf8c4ff73",
   "metadata": {},
   "source": [
    "#### Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "50970e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.0e+03, 3.0e+00, 3.0e+01, 5.0e-01],\n",
       "       [1.0e+03, 4.0e+00, 4.5e+01, 4.0e-01],\n",
       "       [1.5e+03, 1.0e+00, 5.0e+01, 3.0e-01]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_array3 = np.array([[2000,3,30,0.5], [1000,4,45,0.4], [1500,1,50,0.3]])\n",
    "X_array3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "85429f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.22474487e+00  2.67261242e-01 -1.37281295e+00  1.22474487e+00]\n",
      " [-1.22474487e+00  1.06904497e+00  3.92232270e-01  6.79869978e-16]\n",
      " [ 0.00000000e+00 -1.33630621e+00  9.80580676e-01 -1.22474487e+00]]\n"
     ]
    }
   ],
   "source": [
    "# StandardScaler in SciKit Learn\n",
    "std_scaler = StandardScaler().fit(X_array3)\n",
    "normalized_arr = std_scaler.transform(X_array3)\n",
    "print(normalized_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f62cb2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaler mean [1.50000000e+03 2.66666667e+00 4.16666667e+01 4.00000000e-01]\n",
      "scaler std deviation [4.08248290e+02 1.24721913e+00 8.49836586e+00 8.16496581e-02]\n"
     ]
    }
   ],
   "source": [
    "print('scaler mean',std_scaler.mean_)\n",
    "print('scaler std deviation',std_scaler.scale_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6f02e189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.22474487e+00  2.67261242e-01 -1.37281295e+00  1.22474487e+00]\n",
      " [-1.22474487e+00  1.06904497e+00  3.92232270e-01  6.79869978e-16]\n",
      " [ 0.00000000e+00 -1.33630621e+00  9.80580676e-01 -1.22474487e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Our function that is same as StandardScaler\n",
    "X_norm3, avg, stddev = std_norm_v1(X_array3)\n",
    "print(X_norm3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ae442ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our function mean [1.50000000e+03 2.66666667e+00 4.16666667e+01 4.00000000e-01]\n",
      "our function std deviation [4.08248290e+02 1.24721913e+00 8.49836586e+00 8.16496581e-02]\n"
     ]
    }
   ],
   "source": [
    "print('our function mean',avg)\n",
    "print('our function std deviation',stddev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ced9e9",
   "metadata": {},
   "source": [
    "### MinMaxScaler (Normalization Technique)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdbbcbd",
   "metadata": {},
   "source": [
    "In SciKit Learn MinMaxScaler use the formula below:\n",
    "\n",
    "$$x_{scaled} = \\frac{x - min(x)}{max(x)-min(x)}$$\n",
    "$$ $$\n",
    "This effectively scaled the data so that all the data **ranges from 0 to 1 or from -1 to 1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "34f368df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_scaling(X):\n",
    "    \"\"\"\n",
    "    This function is to replicate the same method as sklearn MinMaxScaler\n",
    "    Formula is x(scaled) = x - min(x) / max(x) - min(x)\n",
    "    This function produce similar result as sklearn MinMaxScaler\n",
    "    \"\"\"\n",
    "    maximum = X.max(axis=0)\n",
    "    minimum = X.min(axis=0)\n",
    "    range = (maximum - minimum)\n",
    "    scaled = (X - minimum) / range\n",
    "    return scaled, minimum, range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b35c23-5789-48e9-ade5-b54b9f26f95c",
   "metadata": {},
   "source": [
    "#### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "12c6e171-f21f-4636-8d1c-00151b34eebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         1.         0.         1.        ]\n",
      " [0.45047923 0.33333333 1.         0.5       ]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Our function that is same as MinMaxScaler\n",
    "X_norm1, min, range = minmax_scaling(X_train1)\n",
    "print(X_norm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1d552c90-5aa2-45a0-8622-97da98945a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         1.         0.         1.        ]\n",
      " [0.45047923 0.33333333 1.         0.5       ]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# MinMaxScaler in SciKit Learn\n",
    "minmax_scaler = MinMaxScaler().fit(X_train1)\n",
    "normalized_arr = minmax_scaler.transform(X_train1)\n",
    "print(normalized_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312c4531-1bc7-4d78-a56a-1c9a5dd66894",
   "metadata": {},
   "source": [
    "#### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f96cf991-4219-499c-b1fc-5ebfba0ae742",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_array2 = np.array([2,3,5,6,7,4,8,7,6]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bc9a3a6a-b606-4214-9b23-b6eb9922ff8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.        ]\n",
      " [0.16666667]\n",
      " [0.5       ]\n",
      " [0.66666667]\n",
      " [0.83333333]\n",
      " [0.33333333]\n",
      " [1.        ]\n",
      " [0.83333333]\n",
      " [0.66666667]]\n"
     ]
    }
   ],
   "source": [
    "# MinMaxScaler in SciKit Learn\n",
    "minmax_scaler = MinMaxScaler().fit(x_array2)\n",
    "normalized_arr = minmax_scaler.transform(x_array2)\n",
    "print(normalized_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f1b0009e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.        ]\n",
      " [0.16666667]\n",
      " [0.5       ]\n",
      " [0.66666667]\n",
      " [0.83333333]\n",
      " [0.33333333]\n",
      " [1.        ]\n",
      " [0.83333333]\n",
      " [0.66666667]]\n"
     ]
    }
   ],
   "source": [
    "# Our function that is same as MinMaxScaler\n",
    "x_norm2, min, range = minmax_scaling(x_array2)\n",
    "print(x_norm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca676ce-6191-43ee-b3c3-ddd50336fdf5",
   "metadata": {},
   "source": [
    "#### Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ad4bdb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_array3 = np.array([[2000,3,30,0.5], [1000,4,45,0.4], [1500,1,50,0.3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d403d6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.66666667 0.         1.        ]\n",
      " [0.         1.         0.75       0.5       ]\n",
      " [0.5        0.         1.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# MinMaxScaler in SciKit Learn\n",
    "minmax_scaler = MinMaxScaler().fit(X_array3)\n",
    "normalized_arr = minmax_scaler.transform(X_array3)\n",
    "print(normalized_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9f144d24-8a17-42c5-96fb-d903f708efbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.66666667 0.         1.        ]\n",
      " [0.         1.         0.75       0.5       ]\n",
      " [0.5        0.         1.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# MinMaxScaler in SciKit Learn\n",
    "normalized_arr = MinMaxScaler().fit_transform(X_array3)\n",
    "print(normalized_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0dde5bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.66666667 0.         1.        ]\n",
      " [0.         1.         0.75       0.5       ]\n",
      " [0.5        0.         1.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Our function that is same as MinMaxScaler\n",
    "X_norm3, min, range = minmax_scaling(X_array3)\n",
    "print(X_norm3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4209a185-92ab-4fc3-8e25-cc641aa6360c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Please note that in SciKit Learn feature scaling, we use `fit()` and `transform()`. Alternatively, we can also combined both function using `fit_transform()`. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d71c28-cb5f-4d14-9d88-790e86644024",
   "metadata": {},
   "source": [
    "## Normalization vs Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda8af22-e4c3-4d11-967e-229b30c84b00",
   "metadata": {},
   "source": [
    "The above mention feature scaling techniques are most commonly used. So when should we use normalization (MinMaxScaler) or standardization (StandardScaler)? The main characteristics of both techniques are as follows:\n",
    "\n",
    "**Normalization (MinMaxScaler)**\n",
    "\n",
    "- Use the minimum and maximum of each features for scaling.\n",
    "- Scale all data to a range from 0 to 1. Or scaled the data to a range from -1 to 1.\n",
    "- It is used when all the features have different scale and vary significantly. examples are sensor data or pixel values.\n",
    "- It is affected by outliers\n",
    "- It is useful when we don't know about the distribution.\n",
    "- If the data is not Gaussian distributed, it is best to use MinMaxScaler.\n",
    "- We know that the minimum and maximum number are not outliers and carry important information.\n",
    "- As it is affected by the minimum and maximum number in the dataset, it is best that the data are not skewed and the data are evenly distributed between the minimum and maximum boundary.\n",
    "- In distance based model such as kNN, SVM, and NN; these models are sensitive to feature range and would benefit from scaling. Also best for deep learning models.\n",
    "\n",
    "**Standardization (StandardScaler)**\n",
    "\n",
    "- a.k.a Z-Score Normalization\n",
    "- Use the mean and standard deviation of each feature for scaling.\n",
    "- Scale all data such that the mean is 0 and standard deviation is 1.\n",
    "- If data is Gaussian or Normal distribution, use standardization.\n",
    "- It does not have range boundaries.\n",
    "- It is also affected by outliers but less so compared to normalization.\n",
    "- Algorithm such as logistic regression, linear regression, principal component analysis and linear kernel in SVM; these algorithm assume Gaussian distributed data and therefor it is best to use standardization. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd0dbb8-1973-4d49-b14e-748fa02c81ff",
   "metadata": {},
   "source": [
    "- https://medium.com/@meritshot/standardization-v-s-normalization-6f93225fbd84\n",
    "- https://towardsdatascience.com/normalization-vs-standardization-explained-209e84d0f81e\n",
    "- https://www.secoda.co/learn/when-to-normalize-or-standardize-data\n",
    "- https://www.kdnuggets.com/2020/04/data-transformation-standardization-normalization.html\n",
    "- https://www.geeksforgeeks.org/normalization-vs-standardization/\n",
    "- https://www.simplilearn.com/normalization-vs-standardization-article\n",
    "- https://chatgpt.com/share/66fccc15-ebac-8000-b22a-1cf3847f08ce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b42c4c-1b42-4285-ac66-1f3933aaf857",
   "metadata": {},
   "source": [
    "## Pandas and Numpy Difference in Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b31a96b-3922-46d8-b8a3-6c0483de1a63",
   "metadata": {},
   "source": [
    "**Why The difference?**\n",
    "\n",
    "Short Answer is **Pandas use different formula**.\n",
    "\n",
    "More detailed explanation is as follows:\n",
    "\n",
    "When computing standard deviation, Pandas use the **sample** formula to compute standard deviation using formula as follows:\n",
    "\n",
    "$$s = \\sqrt{\\frac{\\sum\\limits_{i=1}^{n}(x_i - \\overline{x})^2}{n - 1}}$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $s$ is the sample standard deviation\n",
    "- $\\overline{x}$ is the sample mean\n",
    "- $n$ is the total number of observation in the sample\n",
    "\n",
    "For Numpy, it uses standard deviation of **population** as the formula:\n",
    "\n",
    "$$\\sigma = \\sqrt{\\frac{\\sum\\limits_{i=1}^{N}(x_i - \\mu)^2}{N}}$$\n",
    "\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\sigma$ is the population standard deviation\n",
    "- $\\mu$ is the population mean\n",
    "- $N$ is the total number of observation in the population\n",
    "\n",
    "StandardScaler in Scikit Learn uses Numpy Standard Deviation. Our function uses whatever function that the data type belongs to.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77e276f-5a9b-428a-886f-cab1f169448e",
   "metadata": {},
   "source": [
    "### Demonstration of Differences in Scaling Between Pandas and Numpy Using Housing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc46f61-6760-4c9a-a845-e393f7feba91",
   "metadata": {},
   "source": [
    "#### Pandas Calculation Using Housing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "20c9ba77-60e0-452c-85e2-65e048d339a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load housing data\n",
    "df1 = pd.read_csv('./data/housing_one_var.csv')\n",
    "Xsp_train = df1['sqft'].to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "06aa4e25-7ebd-4660-bab5-339f6333610d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_normSp, avgSp, stdSp = std_norm_v1(Xsp_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "33c7f53c-c87b-4168-bcb2-ca6de8308c78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sqft</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.130010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.504190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.502476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.735723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.257476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sqft\n",
       "0  0.130010\n",
       "1 -0.504190\n",
       "2  0.502476\n",
       "3 -0.735723\n",
       "4  1.257476"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_normSp[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "520691fb-9097-4231-9649-e85fc7905853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sqft    2000.680851\n",
       "dtype: float64"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avgSp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "cc57b21d-edaa-4d98-82a6-e799ea161efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sqft    794.702354\n",
       "dtype: float64"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stdSp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d629be70-8f65-4559-8306-8ba58f46749a",
   "metadata": {},
   "source": [
    "#### Numpy Calculation Using Housing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "ff727fce-2802-4b22-9b9b-68b8d5b3bf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdsp_scaler = StandardScaler()\n",
    "normalizedSp_arr = stdsp_scaler.fit_transform(Xsp_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "14402cc0-c0bd-4b5f-bb61-a0c38f538a8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.13141542],\n",
       "       [-0.5096407 ],\n",
       "       [ 0.5079087 ],\n",
       "       [-0.74367706],\n",
       "       [ 1.27107075]])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizedSp_arr[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "514ca40f-e71d-49b3-a088-f9aeffa6bc85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2000.68085106])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stdsp_scaler.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "8b0a1bf3-187a-424c-abd4-9ba1e6643995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([786.20261874])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stdsp_scaler.scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b838b2-e22e-4b0c-a726-f91a94d4ce90",
   "metadata": {},
   "source": [
    "- **Note the difference in standard deviation 794.70 in Pandas vs 786.20 in Numpy. The normalized data is also differs slightly.**\n",
    "- **The standard deviation is different. This is due to the type of data we feed into our own function.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007dba56-1624-4f40-998c-e96a7e887872",
   "metadata": {},
   "source": [
    "### SciKit Learn Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "0ff65442-c436-4586-ab74-9fc8d30ee37f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Xsp_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "343ff727-b8e9-4213-9fea-fcf614f26823",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scaler1 = StandardScaler().fit(Xsp_train)\n",
    "normalized_arr1 = std_scaler1.transform(Xsp_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "62dd5460-e650-4d01-b661-9118c6d25121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([786.20261874])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_scaler1.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "46d7fb90-13fb-41db-afeb-c3aab6ba9c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xsp_train2 = Xsp_train.to_numpy()\n",
    "type(Xsp_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "fd0fb1de-0649-478b-864e-e7bb96ad21c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scaler2 = StandardScaler().fit(Xsp_train2)\n",
    "normalized_arr2 = std_scaler2.transform(Xsp_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "8cd4d2ee-6214-4f9c-9486-d22a1aab68e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([786.20261874])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_scaler2.scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f4e199-9014-4b95-97c6-0dd033be987d",
   "metadata": {},
   "source": [
    "**In SciKit Learn, there is no difference in what type of data type we feed in.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af2b9ba-ee09-41bd-a2d6-399124c5268e",
   "metadata": {},
   "source": [
    "### Manual Computation of Standard Deviation to Confirm Pandas and Numpy Formula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15336c9-18ae-4381-bce3-764b7f65ee79",
   "metadata": {},
   "source": [
    "The following shows the manual computation to replicate the difference in computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "2052b650-8e4d-4f04-998c-188c50f2ee41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sqft    2000.680851\n",
       "dtype: float64"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataMean = Xsp_train.mean()\n",
    "dataMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "36a1dab0-db0e-4b51-9794-8ff4cbff2d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sqft    2.905138e+07\n",
       "dtype: float64"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumOfSquare = (Xsp_train - dataMean) ** 2\n",
    "sumOfSquare = sumOfSquare.sum()\n",
    "sumOfSquare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29938d8-082b-444a-a2e7-d04040219ba9",
   "metadata": {},
   "source": [
    "**Compute Standard Deviation Using Population Formula**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "501d4618-8dba-4c46-bff3-938da8c14ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sqft    618114.557718\n",
       "dtype: float64"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop_std_deviation = sumOfSquare/len(Xsp_train)\n",
    "pop_std_deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "6a396bf3-0c70-4d74-ac05-5ebe9e051f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sqft    786.202619\n",
       "dtype: float64"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Population Standard Deviation\n",
    "np.sqrt(pop_std_deviation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5c2572-bf72-446a-97fb-fa67f473271c",
   "metadata": {},
   "source": [
    "**Compute Standard Deviation Using Sample Formula**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "491eac08-6db2-45e9-8306-1248e9b6baca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sqft    631551.830712\n",
       "dtype: float64"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_std_deviation_pd = sumOfSquare/(len(Xsp_train)-1)\n",
    "sample_std_deviation_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "6b20be5b-011a-4b3f-98ae-047b53e4dac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sqft    794.702354\n",
       "dtype: float64"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "np.sqrt(sample_std_deviation_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8e7acd-b018-4a88-badb-742e5052f495",
   "metadata": {},
   "source": [
    "**The above proves that Pandas use `sample` formula to compute standard deviation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76621876-b4ab-4ed7-aaaf-a9f2d8f238e6",
   "metadata": {},
   "source": [
    "### Pandas and Numpy Computation of Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "8b4060d1-5726-40f0-acbc-1ed5713dbcaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Xsp_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "c0c19d4c-c207-49bf-982e-7b1bbeed6a66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sqft    794.702354\n",
       "dtype: float64"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xsp_train.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "47d82c36-3a52-4474-848b-d6f174efa50e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Xsp_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "b73b6faa-81e1-44a2-bc67-e0211bec5845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "786.2026187430467"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xsp_train2.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06702e2a-325d-40f2-a409-cd8df4bdd78f",
   "metadata": {},
   "source": [
    "As shown above, the standard deviation method comes from the Numpy and Pandas object. Thus the differences occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9d9381-deb8-40bd-83fd-8b0e78b4d87a",
   "metadata": {},
   "source": [
    "### Pandas - Standard Deviation with Degree of Freedom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5529ec3-d408-4955-8ca2-f1335dfbc919",
   "metadata": {},
   "source": [
    "**We can ask Pandas to use population computation by setting the degree of freedom (ddof) to 0. For more information please refer to the links below:**\n",
    "$$$$\n",
    "- https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.std.html\n",
    "- https://towardsdatascience.com/data-normalization-with-pandas-and-scikit-learn-7c1cc6ed6475"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "d76cd4b2-bf29-4c97-bdc1-05f1e641180c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sqft    794.702354\n",
       "dtype: float64"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Below is Pandas Standard Deviation\n",
    "Xsp_train.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "7016e698-ed8a-48a0-ad6f-9e89c3a2b552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "786.2026187430467"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Below is Numpy Standard Deviation\n",
    "Xsp_train2.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "f3eaf962-24d0-4f9c-8360-ed95d218a87c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sqft    786.202619\n",
       "dtype: float64"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xsp_train.std(ddof=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cf2daa-b233-4f12-8712-b3a6078979d7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**As the standard deviation differs slightly, it does not matter very much in term of the objective of machine learning. This section just highlight the differences.**\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebead29b-fe77-4a87-bd47-ee66771894fb",
   "metadata": {},
   "source": [
    "### New Z-Score Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45ac64c-a167-404e-a8c7-de730c281cb7",
   "metadata": {},
   "source": [
    "**Below is the new function for z score that take care of Pandas data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "243daff2-0378-4571-92f8-9f725739f542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_norm_v2(X):\n",
    "    \"\"\"\n",
    "    This function is z-score normalizer.\n",
    "    Formula is x(scaled) = x - mean / {std deviation of x}\n",
    "    There is similar scaler in sklearn is StandardScaler\n",
    "    \"\"\"\n",
    "    ### the following check if data type is Series\n",
    "    ### if is Series convert to data frame\n",
    "    if isinstance(X, pd.Series):\n",
    "        X = X.to_frame()\n",
    "    \n",
    "    avg = X.mean(axis=0)\n",
    "\n",
    "    ### the following check if data type is dataframe\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        std = X.std(axis=0, ddof=0)\n",
    "    else:\n",
    "        std = X.std(axis=0)\n",
    "    \n",
    "    X_norm = (X-avg)/std\n",
    "    \n",
    "    return X_norm, avg, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "dfdc8b8e-f6d8-4f1b-971d-2128dbd73fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feeding pandas dataframe into own Scaling Function\n",
    "x_normDf1, avgDf1, stdDf1 = std_norm_v2(Xsp_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "b51bb694-3210-4ca3-8138-d284c98f38d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sqft</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.131415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.509641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.507909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.743677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.271071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sqft\n",
       "0  0.131415\n",
       "1 -0.509641\n",
       "2  0.507909\n",
       "3 -0.743677\n",
       "4  1.271071"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_normDf1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "24261cd4-53ba-4bea-ad66-364876d8f3fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sqft    786.202619\n",
       "dtype: float64"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stdDf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "c8c28d94-e4b8-4b31-88ec-5a7178f4492e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feeding numpy into own Scaling Function\n",
    "x_norm1, avg1, std1 = std_norm_v2(Xsp_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "7c679fce-1fcc-464a-9618-4e805f84bc33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([786.20261874])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "2222b26e-177e-402e-bcee-73c6cfdadb35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.13141542],\n",
       "       [-0.5096407 ],\n",
       "       [ 0.5079087 ],\n",
       "       [-0.74367706],\n",
       "       [ 1.27107075]])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_norm1[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141b0bc2-7ede-4d55-9d59-ffe98ca68021",
   "metadata": {},
   "source": [
    "## Application of Feature Scaling Using Housing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2729933-de04-44a8-84c4-a70c1f174e78",
   "metadata": {},
   "source": [
    "Feature scaling is not only applicable to gradient descent or regression problem. It is applicable to a variety of machine learning methods.\n",
    "\n",
    "Feature scaling is crucial for machine learning algorithms that are sensitive to the magnitude of data, like gradient based machine learning algorithm such as Regression and Neural Networks. Other distance based machine learning methods such as SVMs and K-Means will also be impacted without feature scaling.\n",
    "\n",
    "Decision tree is one of the few machine learning methods that are not impacted by the scale.\n",
    "\n",
    "The choice of which technique to use depends on the specific machine learning algorithm and the properties of your data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f531e452-8286-45dc-8078-1e7abae80ca1",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "d75bd73a-2eef-4407-9d94-47a9e56d8757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sqft</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2104</td>\n",
       "      <td>399900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1600</td>\n",
       "      <td>329900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2400</td>\n",
       "      <td>369000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1416</td>\n",
       "      <td>232000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3000</td>\n",
       "      <td>539900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sqft   price\n",
       "0  2104  399900\n",
       "1  1600  329900\n",
       "2  2400  369000\n",
       "3  1416  232000\n",
       "4  3000  539900"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/housing_one_var.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "ee96b350-c7c0-4af6-bf77-d9a547ca6175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sqft</th>\n",
       "      <th>rm</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2104</td>\n",
       "      <td>3</td>\n",
       "      <td>399900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1600</td>\n",
       "      <td>3</td>\n",
       "      <td>329900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2400</td>\n",
       "      <td>3</td>\n",
       "      <td>369000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1416</td>\n",
       "      <td>2</td>\n",
       "      <td>232000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3000</td>\n",
       "      <td>4</td>\n",
       "      <td>539900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sqft  rm   price\n",
       "0  2104   3  399900\n",
       "1  1600   3  329900\n",
       "2  2400   3  369000\n",
       "3  1416   2  232000\n",
       "4  3000   4  539900"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv('./data/housing_two_var.txt')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5a808a",
   "metadata": {},
   "source": [
    "## Applying Feature Scaling: Z-Score Scaling on Housing Price Data (One Feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e91b09-fdb4-48c1-8f8f-b77528128c96",
   "metadata": {},
   "source": [
    "### Z-Score Scaling (Housing Data - One Feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "8076dba0-9758-4353-be5e-af88a338130e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train1 = df['sqft']\n",
    "y_train1 = df['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "d6a36f36-7bb7-4503-bd76-f284c9d9e50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xdf_norm1, avg_df1, stddev_df1 = std_norm_v2(x_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "e462f0b7-7946-413e-af93-4b3a95d3b4bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sqft</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.131415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.509641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.507909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.743677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.271071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sqft\n",
       "0  0.131415\n",
       "1 -0.509641\n",
       "2  0.507909\n",
       "3 -0.743677\n",
       "4  1.271071"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xdf_norm1[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "5c06a2ed-ed9b-4524-a44e-2278333ee4fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sqft    2000.680851\n",
       "dtype: float64"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "1a1eb8e1-4a1c-4981-9ace-5dd8d75d8d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sqft    786.202619\n",
       "dtype: float64"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stddev_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "5a9e5e9f-2923-4f62-94e0-75c5844b2776",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train1 = x_train1.to_numpy().reshape(-1,1)\n",
    "y_train1 = y_train1.to_numpy().reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "672a93fc-df31-4284-9bdf-b9d4f8036b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_norm1, avg1, stddev1 = std_norm_v2(x_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "9f3871b9-34e1-4622-bffc-68780c19749f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.13141542],\n",
       "       [-0.5096407 ],\n",
       "       [ 0.5079087 ],\n",
       "       [-0.74367706],\n",
       "       [ 1.27107075]])"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_norm1[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "8f9b5cae-75fc-46cd-bc4c-6e0ef4eb00d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2000.68085106])"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "016e6bee-74f2-4c3c-a302-6cedd751698b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([786.20261874])"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stddev1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "87973495-69c8-418f-885b-1f8c4678e378",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scaler1 = StandardScaler().fit(x_train1)\n",
    "normalized_arr1 = std_scaler1.transform(x_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "a3bbcd07-4c68-4c87-903a-be4e12c3eb91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.13141542],\n",
       "       [-0.5096407 ],\n",
       "       [ 0.5079087 ],\n",
       "       [-0.74367706],\n",
       "       [ 1.27107075]])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_arr1[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "3c6da1ee-6304-4673-bb1a-fb8d3adaa4a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2000.68085106])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_scaler1.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "f23044f3-76dd-4349-b244-34b63acb6657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([786.20261874])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_scaler1.scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067c0aa3-6f76-4fc4-9995-a97ab3f8b55e",
   "metadata": {},
   "source": [
    "### Running Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "2f06ffd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 9999: Last cost = 2.0581e+09: intercept = 3.4041e+05: weights = [[105764.13349281]]\n",
      "best w [[105764.1335]]\n",
      "best b 340412.6596\n"
     ]
    }
   ],
   "source": [
    "coef1, intercept1, _, _, _ = my.compute_gradient_descent(x_norm1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "1b843514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w [[105764.13349282]]\n",
      "b [340412.65957447]\n"
     ]
    }
   ],
   "source": [
    "# SciKit Linear regression with standard scaling\n",
    "std_scaler1 = StandardScaler().fit(x_train1)\n",
    "normalized_arr1 = std_scaler1.transform(x_train1)\n",
    "reg1 = LinearRegression().fit(normalized_arr1, y_train1)\n",
    "print('w',reg1.coef_)\n",
    "print('b',reg1.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "9686ed59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w [[134.52528772]]\n",
      "b [71270.49244873]\n"
     ]
    }
   ],
   "source": [
    "# SciKit Linear Regression without scaling\n",
    "lr1 = LinearRegression().fit(x_train1, y_train1)\n",
    "print('w',lr1.coef_)\n",
    "print('b',lr1.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab134e88",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Please note that the best coefficient differs between scaled and non scaled data.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0270e8cf-be06-4e94-ab23-b2f440f6c43f",
   "metadata": {},
   "source": [
    "### Same Prediction for Scaled and Unscaled Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de95a18-7941-40b9-b7cd-7f12ab55a026",
   "metadata": {},
   "source": [
    "**The following is my prediction using normalized data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "519a6c01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[354311.69781211],\n",
       "       [286510.95280111],\n",
       "       [394131.18297731],\n",
       "       [261758.29986059],\n",
       "       [474846.35560945],\n",
       "       [338303.18857341],\n",
       "       [277632.28381158],\n",
       "       [263238.07802551],\n",
       "       [256915.38950266],\n",
       "       [272251.27230277]])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_predict1 = my.prediction(x_norm1, intercept1, coef1)\n",
    "my_predict1[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9ef161-28e7-46d3-baa8-ce6dcd02904c",
   "metadata": {},
   "source": [
    "**The following is SciKit Learn prediction using normalized data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "d14d41d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[354311.69781212],\n",
       "       [286510.95280112],\n",
       "       [394131.18297731],\n",
       "       [261758.29986059],\n",
       "       [474846.35560945],\n",
       "       [338303.18857341],\n",
       "       [277632.28381158],\n",
       "       [263238.07802551],\n",
       "       [256915.38950266],\n",
       "       [272251.27230277]])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_predict1 = reg1.predict(normalized_arr1)\n",
    "sk_predict1[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8214cf-ef2d-40ab-a992-2bbecea5af93",
   "metadata": {},
   "source": [
    "**The following is SciKit Learn prediction using normal data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "cc75e3f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[354311.69781212],\n",
       "       [286510.95280112],\n",
       "       [394131.18297731],\n",
       "       [261758.29986059],\n",
       "       [474846.35560945],\n",
       "       [338303.18857341],\n",
       "       [277632.28381158],\n",
       "       [263238.07802551],\n",
       "       [256915.38950266],\n",
       "       [272251.27230277]])"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_predict1 = lr1.predict(x_train1)\n",
    "normal_predict1[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15db7ab",
   "metadata": {},
   "source": [
    "**Although the weights are different between scaled and non-scaled data. Prediction should be the same. We can use prediction to compare the results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0325e0f-f0b1-4bca-9871-f4b616021e4d",
   "metadata": {},
   "source": [
    "### Applying One Feature Scaling Using DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "5b648148-ab54-4d79-8cb0-0d3be00209ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train2 = df['sqft'].to_frame()\n",
    "y_train2 = df['price'].to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "32b1afe0-ac8f-404c-a8f7-9852a2293ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_norm2, avg2, stddev2 = std_norm_v2(x_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "d5e417e9-0dc9-47d0-9fbf-24d90bdd1097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sqft    786.202619\n",
       "dtype: float64"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stddev2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "210bd606-94a8-4f92-a242-54f4a98d6915",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scaler2 = StandardScaler().fit(x_train2.values)\n",
    "normalized_arr2 = std_scaler2.transform(x_train2.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "a36c896d-84ae-42f5-b1a6-eb3ce543e98b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([786.20261874])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_scaler2.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "52678c82-c163-4800-8620-e15296e782ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sqft</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.131415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.509641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.507909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.743677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.271071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sqft\n",
       "0  0.131415\n",
       "1 -0.509641\n",
       "2  0.507909\n",
       "3 -0.743677\n",
       "4  1.271071"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_norm2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "ada80d82-10a3-44dc-85d1-539659c768aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.13141542],\n",
       "       [-0.5096407 ],\n",
       "       [ 0.5079087 ],\n",
       "       [-0.74367706],\n",
       "       [ 1.27107075]])"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_arr2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "5c619846-fa5c-46e2-8073-8aea33f9ade8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 9999: Last cost = 2.0581e+09: intercept = 3.4041e+05: weights = [[105764.13349281]]\n",
      "best w [[105764.1335]]\n",
      "best b 340412.6596\n"
     ]
    }
   ],
   "source": [
    "# GD using own function\n",
    "coef2, intercept2, _, _, _ = my.compute_gradient_descent(x_norm2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "256acc90-1ba0-4494-95e3-4d14c96a5022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w [[105764.13349282]]\n",
      "b [340412.65957447]\n"
     ]
    }
   ],
   "source": [
    "# SciKit Learn Regression using scaled data\n",
    "reg2 = LinearRegression().fit(normalized_arr2, y_train2)\n",
    "print('w',reg2.coef_)\n",
    "print('b',reg2.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "38151f5a-5494-4177-8bba-5b8834799b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w [[134.52528772]]\n",
      "b [71270.49244873]\n"
     ]
    }
   ],
   "source": [
    "# SciKit Linear Regression without scaling\n",
    "lr2 = LinearRegression().fit(x_train2, y_train2)\n",
    "print('w',lr2.coef_)\n",
    "print('b',lr2.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815d0e0d-a0a1-468b-ac4d-1b759b933f78",
   "metadata": {},
   "source": [
    "### Predicting Housing Price "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f8c743",
   "metadata": {},
   "source": [
    "#### Predicting Housing Price Using Existing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675e5b8c-13d3-4b65-8eff-904575b3f5eb",
   "metadata": {},
   "source": [
    "Let us use the first data as the form of query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "314491d5-5fcc-42b4-9d67-8d5f49c3fc9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sqft</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sqft\n",
       "0  2104\n",
       "1  1600\n",
       "2  2400\n",
       "3  1416\n",
       "4  3000"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "6000fe52-d9a3-44da-9860-e1651f8f8ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>399900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>329900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>369000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>232000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>539900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    price\n",
       "0  399900\n",
       "1  329900\n",
       "2  369000\n",
       "3  232000\n",
       "4  539900"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "f64f0a28-eca6-49d3-90d0-a661cd6a612d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 9999: Last cost = 2.0581e+09: intercept = 3.4041e+05: weights = [[105764.13349281]]\n",
      "best w [[105764.1335]]\n",
      "best b 340412.6596\n"
     ]
    }
   ],
   "source": [
    "coef2, intercept2, _, _, _ = my.compute_gradient_descent(x_norm2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "c197747b-ec66-4d66-9fdb-88912a2bdb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "myAsk = 2104\n",
    "myAskOne = np.array(myAsk).reshape((1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "40abad33-7905-4d6f-8a4b-38dcd7b86a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.2286815e+08]])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2 = my.prediction(myAskOne, intercept2, coef2)\n",
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "aee60876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted housing price of 2104 sqft is: $222,868,149.53\n"
     ]
    }
   ],
   "source": [
    "print('Predicted housing price of {0} sqft is: ${1:,.2f}'.format(myAsk, result2[0][0])) \n",
    "#array cannot fit into string format use [0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d7562c",
   "metadata": {},
   "source": [
    "The result is off as the predicted price should be around $399,900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "ba209199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>399900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    price\n",
       "0  399900"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train2[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3c9fa7",
   "metadata": {},
   "source": [
    "This is because our data is not normalize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c7e6dd-a86c-4e67-b7dd-1d46d93d4181",
   "metadata": {},
   "source": [
    "#### Using Normalized Data for Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ab5182",
   "metadata": {},
   "source": [
    "To predict with normalize data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "0b81c6c2-a9e2-4bb7-9c84-9732a693d145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.13141542]])"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstData = x_norm2.iloc[0,0]\n",
    "firstData = firstData.reshape((1,1))\n",
    "firstData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "7e90bc35-6816-4ace-9883-34e138cd2db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_predictA = my.prediction(firstData, intercept2, coef2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "eb27064d-ea2f-4e4c-9df1-e09d03547726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[354311.69781211]])"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_predictA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1669f531-c40d-4b63-910a-bd5769f64799",
   "metadata": {},
   "source": [
    "Comparing against prediction without scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "1d230dd5-735b-4cb0-bd46-b009d6d8832c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just predict the first 10 data\n",
    "my_predict2 = lr2.predict(x_train2[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "e4cad649-0c4c-4bb4-891c-9c8c94d9990b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[354311.69781212]])"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result from first prediction\n",
    "my_predict2[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b09ddd-04c5-40ad-9070-4e90e42d4383",
   "metadata": {},
   "source": [
    "#### Using Data not in Existing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908879b1-ee22-4375-bc64-d05b97861fb3",
   "metadata": {},
   "source": [
    "2104 sqft is the first data on our training dataset. What happen if we want to predict numbers that are not in the training set? Let say we want to predict 2800 sqft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "6476a53d-f0b2-4fec-8e27-934fff612756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sqft</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sqft\n",
       "0  2104\n",
       "1  1600\n",
       "2  2400\n",
       "3  1416\n",
       "4  3000"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "e6013632-c549-49c8-bf0e-9c0073f9022a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>399900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>329900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>369000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>232000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>539900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    price\n",
       "0  399900\n",
       "1  329900\n",
       "2  369000\n",
       "3  232000\n",
       "4  539900"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train2[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bf5ec8-2c73-4412-a3cf-f86c7e95404c",
   "metadata": {},
   "source": [
    "Based on the data above, is 2800 sqft house should range between housing price with size of 2400 sqft and 3000 sqft. Thus our prediction should be between 369,000 and 539,900.\n",
    "\n",
    "To normalized new data we need to get the mean and standard deviation used in normalizing the data. This is the reason, we also return the mean and standard deviation in our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "f1faad61-8d48-4b63-b92f-047ed92970a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sqft    2000.680851\n",
       "dtype: float64"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "1e5c07b7-8a06-450d-b573-8707a5633fd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sqft    786.202619\n",
       "dtype: float64"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stddev2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "75883ea7-7c48-47ef-9992-26fcaf8374b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "myAskTwo = np.array([2800]).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "3a69971e-a76c-4618-8b00-4b7bf1c6f3eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myAskTwo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "a7c79d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sqft    1.016683\n",
       "dtype: float64"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myAskTwoNorm = (myAskTwo[0] - avg2) / stddev2\n",
    "myAskTwoNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "76744507-1ed6-4e04-bdcb-07aaa45a1541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([447941.2980654])"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_predictB = my.prediction(myAskTwoNorm, intercept2, coef2)\n",
    "my_predictB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a275c2-a84f-45dd-8445-b33d6e1658a1",
   "metadata": {},
   "source": [
    "Our prediction of 447,941 is between the number 369,000 and 539,900."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf77d41",
   "metadata": {},
   "source": [
    "Please note that for SciKit Learn, the StandardScaler function already remember the scaling factor when we pass the command  \n",
    "\n",
    "`std_scaler = StandardScaler().fit(x_train)`\n",
    "\n",
    "In the command above `std_scaler` is the variable name of the scaler we use on `x_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "331a2bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.96479986e+08]])"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg2.predict(myAskTwo) # still give big figures because our input is not normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d982a32d-8966-4b5b-9469-dbd655586ec2",
   "metadata": {},
   "source": [
    "To convert to scaled data, we use `std_scaler.transform()` to scale the data. The feature scaling variables will remember the average and standard deviation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "014dbc4f-e100-4f04-a527-bb0b20fdd1fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2000.68085106])"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following is the mean previously computed\n",
    "std_scaler2.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "fbff1cb0-0ca1-45d3-b737-ee8583b9c3a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([786.20261874])"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following is the standard deviation previously computed\n",
    "std_scaler2.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "ff10df76",
   "metadata": {},
   "outputs": [],
   "source": [
    "myAskTwoRegNorm = std_scaler2.transform(myAskTwo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "605819c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[447941.2980654]])"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg2.predict(myAskTwoRegNorm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a99d92-f058-4219-a85c-4ed922b87cdd",
   "metadata": {},
   "source": [
    "The following is SciKit Learn non-scaled data, the prediction should be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "bfebd862-5a15-445a-999c-3653e633ffd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w [[134.52528772]]\n",
      "b [71270.49244873]\n"
     ]
    }
   ],
   "source": [
    "# SciKit Linear Regression without scaling\n",
    "lr2 = LinearRegression().fit(x_train2, y_train2)\n",
    "print('w',lr2.coef_)\n",
    "print('b',lr2.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "6483a8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ML310/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[447941.2980654]])"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr2.predict(myAskTwo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20615b93-8a1f-4a69-8c31-980e109ab7b5",
   "metadata": {},
   "source": [
    "The above warning happen because we use Pandas dataframe for training. To avoid this warning, when using SciKit learn use x_train.values instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "e124b633-f51d-4d6f-9bb6-56baedcc65be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w [[134.52528772]]\n",
      "b [71270.49244873]\n"
     ]
    }
   ],
   "source": [
    "# SciKit Linear Regression without scaling\n",
    "lr2 = LinearRegression().fit(x_train2.values, y_train2)\n",
    "print('w',lr2.coef_)\n",
    "print('b',lr2.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "089ada5b-e3f7-492f-9953-ec5bfd45703f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[447941.2980654]])"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr2.predict(myAskTwo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a9b40d",
   "metadata": {},
   "source": [
    "## Applying Feature Scaling: Z Score Scaling on Housing Price Data (Multiple Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "facdd15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train3 = df2[['sqft','rm']]\n",
    "y2_train3 = df2['price']\n",
    "X2_train3 = X2_train3.to_numpy()\n",
    "y2_train3 = y2_train3.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "73924765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w [  139.21067402 -8738.01911233]\n",
      "b 89597.90954279748\n"
     ]
    }
   ],
   "source": [
    "# SciKit Learn Regression without Scaling\n",
    "lr3 = LinearRegression().fit(X2_train3, y2_train3)\n",
    "print('w',lr3.coef_)\n",
    "print('b',lr3.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "c826bf58-5155-4861-87bb-d3562156ffcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 9999: Last cost = 2.0433e+09: intercept = 3.4041e+05: weights = [[109447.79646964  -6578.35485416]]\n",
      "best w [[109447.7965]\n",
      " [ -6578.3549]]\n",
      "best b 340412.6596\n"
     ]
    }
   ],
   "source": [
    "# Own scaling function with own gradient descent function\n",
    "X_norm3, avg3, stddev3 = std_norm_v2(X2_train3)\n",
    "coef3, intercept3, _, _, _ = my.compute_gradient_descent(X_norm3, y2_train3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "1fbb5e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w [109447.79646964  -6578.35485416]\n",
      "b 340412.6595744681\n"
     ]
    }
   ],
   "source": [
    "# SciKit Learn Regression with Scaling\n",
    "std_scaler3 = StandardScaler()\n",
    "normalized_arr3 = std_scaler3.fit_transform(X2_train3)\n",
    "reg3 = LinearRegression().fit(normalized_arr3, y2_train3)\n",
    "print('w',reg3.coef_)\n",
    "print('b',reg3.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "d2ec83b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([356283.1103389 , 286120.93063401, 397489.46984812, 269244.1857271 ,\n",
       "       472277.85514636, 330979.02101847, 276933.02614885, 262037.48402897,\n",
       "       255494.58235014, 271364.59918815])"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_predict3 = lr3.predict(X2_train3)\n",
    "normal_predict3[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "ede0b4ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([356283.1103389 , 286120.93063401, 397489.46984812, 269244.1857271 ,\n",
       "       472277.85514636, 330979.02101847, 276933.02614885, 262037.48402897,\n",
       "       255494.58235014, 271364.59918815])"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_predict3 = reg3.predict(normalized_arr3)\n",
    "sk_predict3[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "1e88c60a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[356283.11033889],\n",
       "       [286120.93063401],\n",
       "       [397489.46984811],\n",
       "       [269244.1857271 ],\n",
       "       [472277.85514636],\n",
       "       [330979.02101847],\n",
       "       [276933.02614885],\n",
       "       [262037.48402896],\n",
       "       [255494.58235014],\n",
       "       [271364.59918814]])"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_predict3 = my.prediction(X_norm3, intercept3, coef3)\n",
    "my_predict3[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "41139484-8797-4560-afd2-45463616fc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg shape: (2,)\n",
      "std dev shape: (2,)\n",
      "intercept shape: ()\n",
      "coef shape: (2, 1)\n"
     ]
    }
   ],
   "source": [
    "print('avg shape:', avg3.shape)\n",
    "print('std dev shape:', stddev3.shape)\n",
    "print('intercept shape:', intercept3.shape)\n",
    "print('coef shape:', coef3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "8b758fb8-bf9a-4dd5-b8f2-1b1320950402",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg3 = avg3.reshape((1,-1))\n",
    "stddev3 = stddev3.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "37815f8e-1929-432c-93cc-4fed4a344f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "myAskThree = np.array([2800, 3]).reshape((1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "28c7815a-a4a0-44fa-a19e-9e85dc54188a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.0166834 , -0.22609337]])"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myAskThreeNorm = (myAskThree - avg3)/stddev3\n",
    "myAskThreeNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "0f83375b-94a1-41c8-8489-9088a6b340ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[453173.73945516]])"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result3 = my.prediction(myAskThreeNorm, intercept3, coef3)\n",
    "result3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "27560507-50fa-4d1b-95da-86c141928c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.0166834 , -0.22609337]])"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myAskThreeNormReg = std_scaler3.transform(myAskThree)\n",
    "myAskThreeNormReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "b579b0ea-3ad3-4fcc-87fc-2311affb997a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([453173.73945517])"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg3.predict(myAskThreeNormReg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "d4125246-50c3-460e-90da-99c2bd854f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([453173.73945517])"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr3.predict(myAskThree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f389395",
   "metadata": {},
   "source": [
    "Similarly, for multiple feature regression, we still cannot be accurate prediction using un-normalized ask. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "e8691ff2-5d7f-40aa-8bd1-ce7ab5e5b51a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.06774508e+08])"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg3.predict(myAskThree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1608f7de-5c7a-4278-8494-cdd7c6f24a40",
   "metadata": {},
   "source": [
    "## Applying Feature Scaling: MinMax Scaling on Housing Price Data (One Feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e62aed-d072-4f52-920e-a20f7a61ab65",
   "metadata": {},
   "source": [
    "### MinMax Scaling (Housing Data - One Feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "fca765ad-d698-4fb0-8ff9-a02c6a5569d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train1b = df['sqft']\n",
    "y_train1b = df['price']\n",
    "x_train1b = x_train1b.to_numpy().reshape(-1,1)\n",
    "y_train1b = y_train1b.to_numpy().reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "f6461597-efb9-438e-9166-ca80a6548a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_norm1b, min1b, range1b = minmax_scaling(x_train1b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "9a659905-af5d-4c64-80bb-576263b64f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.34528406],\n",
       "       [0.20628792],\n",
       "       [0.42691671],\n",
       "       [0.1555433 ],\n",
       "       [0.59238831]])"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_norm1b[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "752359cf-a569-4856-b3ff-5030c045779a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([852])"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "d5a0db54-7f75-4b54-b13e-3bee120f56ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3626])"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "b1705532-9e89-4fe1-9bec-02ee7b4686b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_scaler1b = MinMaxScaler().fit(x_train1b)\n",
    "normalized_arr1b = minmax_scaler1b.transform(x_train1b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "ad39893b-86e7-41d9-b66b-c8a5acc5728b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.34528406],\n",
       "       [0.20628792],\n",
       "       [0.42691671],\n",
       "       [0.1555433 ],\n",
       "       [0.59238831]])"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_arr1b[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "4ce76e56-60e4-45dd-95f3-474fca4270c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4478.])"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minmax_scaler1b.data_max_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "215906d5-85cc-4837-97b8-6a3b8cd90ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([852.])"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minmax_scaler1b.data_min_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "9ff3f717-94d0-430a-ba09-a97113c5ba7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3626.])"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minmax_scaler1b.data_range_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503b18cd-15ec-4990-861a-5db6bbdb4edd",
   "metadata": {},
   "source": [
    "### Running Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "1aa0ba15-b0ab-48e6-8555-fc1f7a529fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 199999: Last cost = 2.0581e+09: intercept = 1.8589e+05: weights = [[487788.69327352]]\n",
      "best w [[487788.6933]]\n",
      "best b 185886.0376\n"
     ]
    }
   ],
   "source": [
    "coef1b, intercept1b, _, _, _ = my.compute_gradient_descent(x_norm1b, y_train1b, iterations=200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "ab43f5e5-5bc8-4e61-9714-437fc05295d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w [[487788.6932736]]\n",
      "b [185886.03758637]\n"
     ]
    }
   ],
   "source": [
    "# SciKit Linear regression with MinMax scaling\n",
    "minmax_scaler1b = MinMaxScaler().fit(x_train1b)\n",
    "normalized_arr1b = minmax_scaler1b.transform(x_train1b)\n",
    "reg1b = LinearRegression().fit(normalized_arr1b, y_train1b)\n",
    "print('w',reg1b.coef_)\n",
    "print('b',reg1b.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "a946496b-fa9e-4c1e-9fd3-7d00c65b17e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w [[134.52528772]]\n",
      "b [71270.49244873]\n"
     ]
    }
   ],
   "source": [
    "# SciKit Linear Regression without scaling\n",
    "lr1b = LinearRegression().fit(x_train1b, y_train1b)\n",
    "print('w',lr1b.coef_)\n",
    "print('b',lr1b.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cce5eb-58f7-47fc-bd17-270737c93118",
   "metadata": {},
   "source": [
    "Please note that the best coefficient differs between scaled and non scaled data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db693d7-8106-4eb5-8b0c-e52a207a33a1",
   "metadata": {},
   "source": [
    "### Same Prediction for Scaled and Unscaled Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbbd704-a36a-4a00-acde-97e5dbe678b0",
   "metadata": {},
   "source": [
    "**The following is my prediction using normalized data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "09c47559-3e04-4e75-a715-70423c39d958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[354311.69781212],\n",
       "       [286510.95280112],\n",
       "       [394131.1829773 ],\n",
       "       [261758.2998606 ],\n",
       "       [474846.35560943],\n",
       "       [338303.18857341],\n",
       "       [277632.28381159],\n",
       "       [263238.07802553],\n",
       "       [256915.38950268],\n",
       "       [272251.27230278]])"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_predict1b = my.prediction(x_norm1b, intercept1b, coef1b)\n",
    "my_predict1b[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb4e1d4-80dd-44f1-8334-393ff169c5b4",
   "metadata": {},
   "source": [
    "**The following is SciKit Learn prediction using normalized data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "28467ead-420e-4758-bc0c-372ce4f0aaaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[354311.69781212],\n",
       "       [286510.95280112],\n",
       "       [394131.18297731],\n",
       "       [261758.29986059],\n",
       "       [474846.35560945],\n",
       "       [338303.18857341],\n",
       "       [277632.28381158],\n",
       "       [263238.07802551],\n",
       "       [256915.38950266],\n",
       "       [272251.27230277]])"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_predict1b = reg1b.predict(normalized_arr1b)\n",
    "sk_predict1b[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d746f838-92dc-4bda-8b1a-6a641a7a211f",
   "metadata": {},
   "source": [
    "**The following is SciKit Learn prediction using normal data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "ae261998-6a6b-48db-8ae3-cd3556ee47b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[354311.69781212],\n",
       "       [286510.95280112],\n",
       "       [394131.18297731],\n",
       "       [261758.29986059],\n",
       "       [474846.35560945],\n",
       "       [338303.18857341],\n",
       "       [277632.28381158],\n",
       "       [263238.07802551],\n",
       "       [256915.38950266],\n",
       "       [272251.27230277]])"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_predict1b = lr1b.predict(x_train1)\n",
    "normal_predict1b[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0bc8ee-1439-437e-8840-83cefa8195fa",
   "metadata": {},
   "source": [
    "**Although the weights are different between scaled and non-scaled data. Prediction should be the same. We can use prediction to compare the results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3882e2-425a-4bb1-bb35-55c25bbef489",
   "metadata": {},
   "source": [
    "### Predicting Housing Price "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4248cd-7927-4063-809f-1de91360ba07",
   "metadata": {},
   "source": [
    "#### Predicting Housing Price Using Existing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "55fc9433-14f6-4508-9a47-65243caa6fb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[487788.69327352]])"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "c99f9cea-346e-4c3d-b082-0491994d1369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185886.03758639883"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intercept1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "ee268a69-4f8e-41ed-a75b-5e58da6f7dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "myAsk = 2104\n",
    "myAskOne = np.array(myAsk).reshape((1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "500934aa-5b0d-4383-a1d7-bf446042b4be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0264933e+09]])"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1b = my.prediction(myAskOne, intercept1b, coef1b)\n",
    "result1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "5a6c9020-fdc0-40c3-b0b7-75ac203e47b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted housing price of 2104 sqft is: $1,026,493,296.69\n"
     ]
    }
   ],
   "source": [
    "print('Predicted housing price of {0} sqft is: ${1:,.2f}'.format(myAsk, result1b[0][0])) \n",
    "#array cannot fit into string format use [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "7e32f39e-e5da-4cbf-9626-ca3766b78401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([399900])"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train1[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe809d91-863a-46d2-9a29-12e67638f390",
   "metadata": {},
   "source": [
    "This is because our data is not normalize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86feb78-cdf5-4321-88ec-3ef86dc1970b",
   "metadata": {},
   "source": [
    "#### Using Normalized Data for Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28879767-088e-4156-a7a3-edfbcc8c83cd",
   "metadata": {},
   "source": [
    "To predict with normalize data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "8ea2eddd-4c64-4cdf-94a7-2078f9ce6a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.34528406]])"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstData = normalized_arr1b[0]\n",
    "firstData = firstData.reshape((1,1))\n",
    "firstData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "43865b81-bf7b-49f7-89ba-593d88753c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[354311.69781212]])"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_predictA1 = my.prediction(firstData, intercept1b, coef1b)\n",
    "my_predictA1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c9e9be-fad0-4708-b51f-a08df1e396fd",
   "metadata": {},
   "source": [
    "Comparing against prediction without scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "7cc998c3-1de2-40a9-9dc5-78cac80c743a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just predict the first 10 data\n",
    "my_predictA2 = lr1b.predict(x_train1b[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "c3516765-2ea7-427e-b971-d938a02d5b51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[354311.69781212]])"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result from first prediction\n",
    "my_predictA2[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e06af4f-08dd-4e93-a858-0718bf455078",
   "metadata": {},
   "source": [
    "#### Using Data not in Existing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b9802f-ed25-446a-8e1d-0b70711c1097",
   "metadata": {},
   "source": [
    "Based on the dataset, 2800 sqft house should  range should be between 2400 sqft and 3000 sqft. Thus our prediction should be between 369,000 and 539,900.\n",
    "\n",
    "To normalized new data we need to get the minimum and the range between maximum and minimum used in normalizing the data. This is the reason, we also return the minimum and range in our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "08df4804-c557-4ffd-9776-7960776bae6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "myAskTwo = np.array([2800]).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "f8a04d78-1609-4004-b475-abb47293a880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.53723111])"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myAskTwoNorm = (myAskTwo[0] - min1b) / range1b\n",
    "myAskTwoNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "af2a4b88-e70f-4de9-8b51-a74833456199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([447941.29806539])"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_predictB = my.prediction(myAskTwoNorm, intercept1b, coef1b)\n",
    "my_predictB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9c9e58-6dab-48b5-afca-e9cb9758d32a",
   "metadata": {},
   "source": [
    "Our prediction of 447,941 is between the number 369,000 and 539,900."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e985ee-6f19-40e8-acc5-34830492aca8",
   "metadata": {},
   "source": [
    "For SciKit Learn, the MinMaxScaler() will remember the variable. To convert to scaled data, we use `minmax_scaler.transform()` to scale the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "3a4d00ec-c79b-43a0-a46a-67a4bdad4855",
   "metadata": {},
   "outputs": [],
   "source": [
    "myAskTwoRegNorm = minmax_scaler1b.transform(myAskTwo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "1f5b920e-12c5-43c9-babc-44a3523f11b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[447941.2980654]])"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg1b.predict(myAskTwoRegNorm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b294dd9-b33a-4797-ac2d-0b47da6b0a31",
   "metadata": {},
   "source": [
    "The following is SciKit Learn non-scaled data, the prediction should be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "87bb8ba0-0673-43b5-a42f-1623cc96986e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[447941.2980654]])"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr1b.predict(myAskTwo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a9856a-b8da-4d05-b6e7-2e28e400751b",
   "metadata": {},
   "source": [
    "## Applying Feature Scaling: MinMax Scaling on Housing Price Data (Multiple Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "147f4dcc-48e9-401d-8fcb-4e533ad4f1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w [  139.21067402 -8738.01911233]\n",
      "b 89597.90954279748\n"
     ]
    }
   ],
   "source": [
    "# SciKit Learn Regression without Scaling\n",
    "lr2b = LinearRegression().fit(X2_train3, y2_train3)\n",
    "print('w',lr2b.coef_)\n",
    "print('b',lr2b.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "76b6aea0-802e-4d37-8499-8b39479e695f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 199999: Last cost = 2.0433e+09: intercept = 1.9947e+05: weights = [[504777.90398781 -34952.07644922]]\n",
      "best w [[504777.904 ]\n",
      " [-34952.0764]]\n",
      "best b 199467.3847\n"
     ]
    }
   ],
   "source": [
    "# Own scaling function with own gradient descent function\n",
    "X_norm2b, min2b, range2b = minmax_scaling(X2_train3)\n",
    "coef2b, intercept2b, _, _, _ = my.compute_gradient_descent(X_norm2b, y2_train3, iterations=200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "73437ddd-8614-4fa7-ae7b-96c30d590fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w [504777.90398791 -34952.07644931]\n",
      "b 199467.38469348656\n"
     ]
    }
   ],
   "source": [
    "# SciKit Learn Regression with Scaling\n",
    "minmax_scaler2b = MinMaxScaler()\n",
    "normalized_arr2b = minmax_scaler2b.fit_transform(X2_train3)\n",
    "reg2b = LinearRegression().fit(normalized_arr2b, y2_train3)\n",
    "print('w',reg2b.coef_)\n",
    "print('b',reg2b.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f6801e-5d0a-4909-8e30-1cab08b80730",
   "metadata": {},
   "source": [
    "#### Predicting Housing Price Using Existing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "25b6e3b5-040d-4e2b-b7d2-2e285f8e9172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([356283.1103389 , 286120.93063401, 397489.46984812, 269244.1857271 ,\n",
       "       472277.85514636, 330979.02101847, 276933.02614885, 262037.48402897,\n",
       "       255494.58235014, 271364.59918815])"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_predict2b = lr2b.predict(X2_train3)\n",
    "normal_predict2b[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "40c4efe3-8bcc-4603-a76e-a33d19550efb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([356283.1103389 , 286120.93063401, 397489.46984812, 269244.1857271 ,\n",
       "       472277.85514636, 330979.02101847, 276933.02614885, 262037.48402897,\n",
       "       255494.58235014, 271364.59918815])"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_predict2b = reg2b.predict(normalized_arr2b)\n",
    "sk_predict2b[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "6bffebc6-57ee-4c92-b289-c52084d1645f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[356283.11033889],\n",
       "       [286120.93063402],\n",
       "       [397489.4698481 ],\n",
       "       [269244.18572709],\n",
       "       [472277.85514635],\n",
       "       [330979.02101849],\n",
       "       [276933.02614886],\n",
       "       [262037.48402898],\n",
       "       [255494.58235015],\n",
       "       [271364.59918815]])"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_predict2b = my.prediction(X_norm2b, intercept2b, coef2b)\n",
    "my_predict2b[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1c4e27-d486-4206-ae82-02a0f6fe92db",
   "metadata": {},
   "source": [
    "#### Using Data not in Existing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "a3c15673-000e-45ed-a923-ffcf8bc1b9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "min2b = min2b.reshape((1,-1))\n",
    "range2b = range2b.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "d0dcbf5f-1d21-4cab-9a4f-c9c99fabbcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "myAskThree = np.array([2800, 3]).reshape((1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "e7ab5986-a0cc-4821-9eec-c55b4edac8f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.53723111, 0.5       ]])"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myAskThreeNorm = (myAskThree - min2b)/range2b\n",
    "myAskThreeNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "0863e4ad-10b4-43bc-bced-5057f3ec95b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[453173.73945514]])"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2b = my.prediction(myAskThreeNorm, intercept2b, coef2b)\n",
    "result2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "eab99e64-5b45-429a-99a2-3cdacdff4c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.53723111, 0.5       ]])"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myAskThreeNormReg = minmax_scaler2b.transform(myAskThree)\n",
    "myAskThreeNormReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "80c62b91-fe25-4a99-9ba6-abd690abeb33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([453173.73945517])"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg2b.predict(myAskThreeNormReg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "c5e358b5-1f4a-42e0-8df1-5a31a71074ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([453173.73945517])"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr3.predict(myAskThree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb02be7a-4e90-4b77-b656-e14f6561b642",
   "metadata": {},
   "source": [
    "Similarly, for multiple feature regression, we still cannot be accurate prediction using un-normalized ask. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "a50a003c-0c13-44de-8534-4905c72b11e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.41347274e+09])"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg2b.predict(myAskThree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec62fd1-94fe-4368-b288-f49e2e9b0bca",
   "metadata": {},
   "source": [
    "## End Note 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3416cfc-e3d4-44c9-b2b5-a4ee317f85cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
